import os
import json
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Literal, TypedDict, Annotated, Optional

# --- LANGCHAIN & LANGGRAPH ---
from langchain_core.language_models import BaseChatModel
from langchain_core.tools import tool, BaseTool
from langchain_core.messages import BaseMessage, SystemMessage, AIMessage, RemoveMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

# --- PROVIDERS ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

# --- CONFIG & UTILS ---
from pydantic import Field, SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict
from dotenv import load_dotenv

# --- LOCAL MODULES ---
# 1. LOGGING (–ü–æ–¥–∫–ª—é—á–∞–µ–º –≤–∞—à –º–æ–¥—É–ª—å)
try:
    from logging_config import setup_logging
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–≥–µ—Ä —á–µ—Ä–µ–∑ –≤–∞—à –∫–æ–Ω—Ñ–∏–≥ (Rich + File + Filters)
    logger = setup_logging() 
except ImportError:
    # Fallback –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("agent")
    logger.warning("logging_config.py not found, using default logging.")

# 2. FILE TOOLS
try:
    from delete_tools import SafeDeleteFileTool, SafeDeleteDirectoryTool
except ImportError:
    SafeDeleteFileTool = SafeDeleteDirectoryTool = None

# 3. MCP CLIENT
try:
    from langchain_mcp_adapters.client import MultiServerMCPClient
except ImportError:
    MultiServerMCPClient = None


# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø (Pydantic)
# ==========================================

class AgentConfig(BaseSettings):
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞. –ß–∏—Ç–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞ .env
    """
    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', extra='ignore')

    provider: Literal["gemini", "openai"] = "gemini"
    
    # API Keys & Models
    gemini_api_key: Optional[SecretStr] = None
    gemini_model: str = "gemini-1.5-flash"
    
    openai_api_key: Optional[SecretStr] = None
    openai_model: str = "gpt-4o"
    openai_base_url: Optional[str] = None

    # LLM Settings
    temperature: float = 0.5
    max_retries: int = 3
    
    # Agent Logic Settings
    use_long_term_memory: bool = Field(default=False, alias="LONG_TERM_MEMORY")
    summary_threshold: int = Field(default=15, alias="SESSION_SIZE")
    
    # Paths
    mcp_config_path: Path = Path("mcp.json")
    prompt_path: Path = Path("prompt.txt")
    memory_db_path: str = "./memory_db"

    def get_llm(self) -> BaseChatModel:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
        if self.provider == "gemini":
            if not self.gemini_api_key:
                raise ValueError("GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
            return ChatGoogleGenerativeAI(
                model=self.gemini_model,
                temperature=self.temperature,
                google_api_key=self.gemini_api_key.get_secret_value(),
                max_retries=self.max_retries,
                convert_system_message_to_human=True
            )
        elif self.provider == "openai":
            if not self.openai_api_key:
                raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
            return ChatOpenAI(
                model=self.openai_model,
                temperature=self.temperature,
                api_key=self.openai_api_key.get_secret_value(),
                base_url=self.openai_base_url,
                max_retries=self.max_retries,
                model_kwargs={"stream_options": {"include_usage": True}}
            )
        raise ValueError(f"Unknown provider: {self.provider}")


# ==========================================
# 2. –°–û–°–¢–û–Ø–ù–ò–ï –ì–†–ê–§–ê
# ==========================================

class AgentState(TypedDict):
    """
    messages: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è).
    summary: –°–∂–∞—Ç–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    messages: Annotated[list[BaseMessage], add_messages]
    summary: str


# ==========================================
# 3. WORKFLOW –ê–ì–ï–ù–¢–ê
# ==========================================

class AgentWorkflow:
    def __init__(self):
        load_dotenv()
        self.config = AgentConfig()
        self.tools: List[BaseTool] = []
        self.llm: Optional[BaseChatModel] = None
        self.llm_with_tools: Optional[BaseChatModel] = None
        self._cached_prompt: Optional[str] = None

    @staticmethod
    def _messages_to_summary_text(messages: List[BaseMessage], *, per_message_limit: int = 800, total_limit: int = 6000) -> str:
        parts: List[str] = []
        total = 0
        for m in messages:
            role = getattr(m, "type", m.__class__.__name__)
            content = getattr(m, "content", "")
            if isinstance(content, list):
                content_str = "".join(
                    (x.get("text", "") if isinstance(x, dict) else str(x))
                    for x in content
                )
            else:
                content_str = str(content)

            content_str = content_str.strip()
            if len(content_str) > per_message_limit:
                content_str = content_str[:per_message_limit] + "..."

            chunk = f"{role}: {content_str}".strip()
            if not chunk:
                continue

            if total + len(chunk) + 1 > total_limit:
                break
            parts.append(chunk)
            total += len(chunk) + 1

        return "\n".join(parts)

    async def initialize_resources(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ (LLM, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –ø–∞–º—è—Ç—å)."""
        logger.info(f"Initializing agent with provider: [bold cyan]{self.config.provider}[/]", extra={"markup": True})
        self.llm = self.config.get_llm()

        # 1. –§–∞–π–ª–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (–µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–∞–π–¥–µ–Ω)
        if SafeDeleteFileTool and SafeDeleteDirectoryTool:
            cwd = Path.cwd()
            self.tools.extend([
                SafeDeleteFileTool(root_dir=cwd),
                SafeDeleteDirectoryTool(root_dir=cwd)
            ])
            logger.info("File system tools loaded (Sandbox enabled).")

        # 2. –î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å
        if self.config.use_long_term_memory:
            self._init_memory_tools()

        # 3. MCP Tools (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥ –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
        if MultiServerMCPClient and self.config.mcp_config_path.exists():
            await self._init_mcp_tools()

        # –ü—Ä–∏–≤—è–∑–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∫ LLM
        if self.tools:
            self.llm_with_tools = self.llm.bind_tools(self.tools)
        else:
            self.llm_with_tools = self.llm

    def _init_memory_tools(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø–∞–º—è—Ç–∏ (Recall, Remember, Forget)."""
        try:
            from memory_manager import MemoryManager
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä (Singleton)
            memory = MemoryManager(db_path=self.config.memory_db_path)
            
            @tool
            async def remember_fact(text: str, category: str = "general") -> str:
                """
                –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∂–Ω—ã–π —Ñ–∞–∫—Ç –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ, –ø—Ä–æ–µ–∫—Ç–µ –∏–ª–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö.
                """
                return await memory.aremember(text, {"type": category})

            @tool
            async def recall_facts(query: str) -> str:
                """
                –ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø–æ —Å–º—ã—Å–ª–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É.
                """
                facts = await memory.arecall(query)
                return "\n".join(f"- {f}" for f in facts) if facts else "–í –ø–∞–º—è—Ç–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

            @tool
            async def forget_fact(query: str) -> str:
                """
                –£–¥–∞–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–∞–º—è—Ç–∏. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –∑–∞–±—ã—Ç—å —á—Ç–æ-—Ç–æ,
                –∏–ª–∏ –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å—Ç–∞–ª–∞ –Ω–µ–≤–µ—Ä–Ω–æ–π.
                """
                try:
                    count = await memory.adelete_fact_by_query(query)
                    if count > 0:
                        return f"–£—Å–ø–µ—à–Ω–æ –∑–∞–±—ã—Ç–æ —Ñ–∞–∫—Ç–æ–≤: {count}"
                    return "–§–∞–∫—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
                except Exception as e:
                    return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}"

            self.tools.extend([remember_fact, recall_facts, forget_fact])
            logger.info("Memory tools loaded (Remember, Recall, Forget).")
        except ImportError:
            logger.warning("MemoryManager module not found. Memory tools disabled.")
        except Exception as e:
            logger.error(f"Error loading memory tools: {e}")

    async def _init_mcp_tools(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ Model Context Protocol (MCP)."""
        if not self.config.mcp_config_path.exists():
            return

        try:
            raw_cfg = json.loads(self.config.mcp_config_path.read_text("utf-8"))
            
            mcp_cfg = {}
            for name, config in raw_cfg.items():
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –≤—ã–∫–ª—é—á–µ–Ω–æ
                if not config.get("enabled", True):
                    continue
                
                # –°–æ–∑–¥–∞–µ–º —á–∏—Å—Ç—É—é –∫–æ–ø–∏—é –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ MCP
                clean_config = config.copy()
                
                # –£–î–ê–õ–Ø–ï–ú –∫–ª—é—á 'enabled', —á—Ç–æ–±—ã –∫–ª–∏–µ–Ω—Ç MCP –Ω–µ —Ä—É–≥–∞–ª—Å—è
                clean_config.pop("enabled", None)
                
                # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Ç–∏
                current_args = clean_config.get("args", [])
                clean_config["args"] = [
                    arg.replace("{filesystem_path}", str(Path.cwd())) 
                    for arg in current_args
                ]
                
                mcp_cfg[name] = clean_config
            
            if mcp_cfg:
                client = MultiServerMCPClient(mcp_cfg)
                if hasattr(asyncio, "timeout"):
                    async with asyncio.timeout(60):
                        new_tools = await client.get_tools()
                else:
                    new_tools = await asyncio.wait_for(client.get_tools(), timeout=60)

                self.tools.extend(new_tools)
                logger.info(f"Loaded {len(new_tools)} MCP tools from: {list(mcp_cfg.keys())}")
        except Exception as e:
            logger.error(f"MCP Load Error: {e}")

    def _get_system_prompt(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —à–∞–±–ª–æ–Ω–∞."""
        if not self._cached_prompt:
            if self.config.prompt_path.exists():
                self._cached_prompt = self.config.prompt_path.read_text("utf-8")
            else:
                self._cached_prompt = "Role: AI Assistant. Be helpful."

        now = datetime.now()
        prompt = self._cached_prompt.replace("{{current_date}}", now.strftime("%Y-%m-%d (%A)"))
        prompt = prompt.replace("{{cwd}}", str(Path.cwd()))
        return prompt

    # ==========================================
    # 4. –£–ó–õ–´ –ì–†–ê–§–ê (NODES)
    # ==========================================

    async def _summarize_node(self, state: AgentState):
        """
        –£–∑–µ–ª —Å–∂–∞—Ç–∏—è –∏—Å—Ç–æ—Ä–∏–∏. –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –∏—Å—Ç–æ—Ä–∏—è –≤—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å HumanMessage.
        """
        messages = state["messages"]
        summary = state.get("summary", "")

        if not self.llm:
            return {}

        if len(messages) > self.config.summary_threshold:
            # 1. –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –º—ã –•–û–¢–ò–ú –æ—Å—Ç–∞–≤–∏—Ç—å
            keep_last = 4 
            
            # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Ç–∞–∫ –º–∞–ª–æ, –≤—ã—Ö–æ–¥–∏–º
            if len(messages) <= keep_last:
                return {}

            # 3. –£–º–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –≥—Ä–∞–Ω–∏—Ü—ã:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –û–°–¢–ê–ù–ï–¢–°–Ø (messages[-keep_last]).
            # –ï—Å–ª–∏ –æ–Ω–æ –ù–ï HumanMessage, –Ω–∞–º –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –∏ –µ–≥–æ —Ç–æ–∂–µ.
            # –ú—ã —Å–¥–≤–∏–≥–∞–µ–º –≥—Ä–∞–Ω–∏—Ü—É –≤–ø—Ä–∞–≤–æ, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–º HumanMessage –∏–ª–∏ –ø–æ–∫–∞ –Ω–µ –∫–æ–Ω—á–∞—Ç—Å—è —Å–æ–æ–±—â–µ–Ω–∏—è.
            
            idx_start_keep = len(messages) - keep_last
            
            while idx_start_keep < len(messages):
                msg = messages[idx_start_keep]
                if isinstance(msg, HumanMessage):
                    break # –ù–∞—à–ª–∏ –Ω–∞—á–∞–ª–æ –¥–∏–∞–ª–æ–≥–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, –≤—Å—ë –æ–∫
                
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å (AI –∏–ª–∏ Tool), —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ —Ç–æ–∂–µ –Ω–∞–¥–æ —Å–∂–∞—Ç—å/—É–¥–∞–ª–∏—Ç—å
                idx_start_keep += 1
            
            # –ï—Å–ª–∏ –º—ã –¥–æ—à–ª–∏ –¥–æ –∫–æ–Ω—Ü–∞ –∏ –Ω–µ –Ω–∞—à–ª–∏ HumanMessage, –∑–Ω–∞—á–∏—Ç —É–¥–∞–ª—è–µ–º –≤–æ–æ–±—â–µ –≤—Å—ë
            # (—ç—Ç–æ –ª—É—á—à–µ, —á–µ–º –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –±–∏—Ç—É—é –∏—Å—Ç–æ—Ä–∏—é)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ
            to_summarize = messages[:idx_start_keep]
            
            if not to_summarize:
                return {}

            to_summarize_text = self._messages_to_summary_text(to_summarize)

            prompt = (
                f"Current summary: {summary}\n"
                f"New interactions:\n{to_summarize_text}\n\n"
                "Create a concise updated summary of the conversation, preserving key facts and user requests."
            )
            
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∞–º–º–∞—Ä–∏
                res = await self.llm.ainvoke(prompt)
                
                # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
                delete_msgs = [RemoveMessage(id=m.id) for m in to_summarize if m.id]
                
                logger.info(f"Summarized context. Removed {len(delete_msgs)} messages. New history starts with Human.")
                return {"summary": res.content, "messages": delete_msgs}
            except Exception as e:
                logger.error(f"Summarization failed: {e}")
                return {}
        
        return {}
        
    async def _agent_node(self, state: AgentState):
        """
        –ì–ª–∞–≤–Ω—ã–π —É–∑–µ–ª –∞–≥–µ–Ω—Ç–∞.
        """
        if not self.llm_with_tools:
            raise RuntimeError("AgentWorkflow is not initialized. Call initialize_resources() before build_graph()/run.")

        messages = state["messages"]
        summary = state.get("summary", "")
        
        sys_text = self._get_system_prompt()
        if summary:
            sys_text += f"\n\n### Context Summary\n{summary}"
        
        sys_msg = SystemMessage(content=sys_text)
        user_history = [m for m in messages if not isinstance(m, SystemMessage)]
        
        final_messages = [sys_msg] + user_history
        
        response = await self.llm_with_tools.ainvoke(final_messages)
        return {"messages": [response]}

    # ==========================================
    # 5. –°–ë–û–†–ö–ê –ì–†–ê–§–ê
    # ==========================================

    def build_graph(self):
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è –≥—Ä–∞—Ñ–∞ LangGraph."""
        workflow = StateGraph(AgentState)

        workflow.add_node("summarize", self._summarize_node)
        workflow.add_node("agent", self._agent_node)
        
        if self.tools:
            workflow.add_node("tools", ToolNode(self.tools))

        workflow.add_edge(START, "summarize")
        workflow.add_edge("summarize", "agent")

        def should_continue(state):
            last_msg = state["messages"][-1]
            tool_calls = getattr(last_msg, "tool_calls", None)
            return "tools" if tool_calls else END

        workflow.add_conditional_edges(
            "agent",
            should_continue,
            ["tools", END] if self.tools else [END]
        )

        if self.tools:
            workflow.add_edge("tools", "agent")

        return workflow.compile(checkpointer=MemorySaver())


# ==========================================
# 6. –¢–û–ß–ö–ê –í–•–û–î–ê (TEST)
# ==========================================

if __name__ == "__main__":
    async def main():
        print("--- Testing Agent Initialization ---")
        try:
            wf = AgentWorkflow()
            await wf.initialize_resources()
            app = wf.build_graph()
            print("‚úÖ Agent Graph built successfully.")
            print(f"üîß Tools: {[t.name for t in wf.tools]}")
        except Exception as e:
            print(f"‚ùå Initialization failed: {e}")

    asyncio.run(main())